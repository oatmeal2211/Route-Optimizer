{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006b8674",
   "metadata": {},
   "source": [
    "# HERE Traffic API Data Extraction\n",
    "\n",
    "This notebook demonstrates how to fetch traffic data from the HERE Traffic API v7 and prepare it for merging with map data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6ad6",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e327ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.1)\n",
      "Requirement already satisfied: geopandas in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.3)\n",
      "Collecting folium\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geopandas) (24.0)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.1.2)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from folium) (3.1.4)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2>=2.9->folium) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maxyj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "Downloading branca-0.8.2-py3-none-any.whl (26 kB)\n",
      "Downloading xyzservices-2025.4.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: xyzservices, branca, folium\n",
      "\n",
      "   ---------------------------------------- 0/3 [xyzservices]\n",
      "   ------------- -------------------------- 1/3 [branca]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   -------------------------- ------------- 2/3 [folium]\n",
      "   ---------------------------------------- 3/3 [folium]\n",
      "\n",
      "Successfully installed branca-0.8.2 folium-0.20.0 xyzservices-2025.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\maxyj\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests pandas geopandas matplotlib folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6269d3",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5d09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from datetime import datetime\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "if not os.path.exists('cache'):\n",
    "    os.makedirs('cache')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7809b",
   "metadata": {},
   "source": [
    "## Step 3: Set Up HERE API Credentials\n",
    "\n",
    "You need to:\n",
    "1. Sign up at [HERE Developer Portal](https://developer.here.com/)\n",
    "2. Create a project and generate an API key\n",
    "3. Replace `YOUR_API_KEY` below with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff279cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API configuration set!\n"
     ]
    }
   ],
   "source": [
    "# HERE API Configuration\n",
    "HERE_API_KEY = HERE_API_KEY  # Replace with your actual API key\n",
    "\n",
    "# HERE Traffic API v7 Endpoints\n",
    "TRAFFIC_FLOW_URL = \"https://data.traffic.hereapi.com/v7/flow\"\n",
    "TRAFFIC_INCIDENTS_URL = \"https://data.traffic.hereapi.com/v7/incidents\"\n",
    "\n",
    "print(\"API configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37938a45",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions for API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eebc6a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def get_cache_filename(url, params):\n",
    "    \"\"\"Generate a unique cache filename based on request parameters\"\"\"\n",
    "    cache_key = f\"{url}_{json.dumps(params, sort_keys=True)}\"\n",
    "    hash_key = hashlib.sha1(cache_key.encode()).hexdigest()\n",
    "    return f\"cache/{hash_key}.json\"\n",
    "\n",
    "def fetch_traffic_flow(bbox, api_key, use_cache=True):\n",
    "    \"\"\"\n",
    "    Fetch traffic flow data for a bounding box\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox: tuple (west, south, east, north) in WGS84 coordinates\n",
    "    - api_key: HERE API key\n",
    "    - use_cache: whether to use cached data\n",
    "    \n",
    "    Returns:\n",
    "    - JSON response with traffic flow data\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'apiKey': api_key,\n",
    "        'in': f'bbox:{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}',\n",
    "        'locationReferencing': 'shape'  # Include geometry\n",
    "    }\n",
    "    \n",
    "    cache_file = get_cache_filename(TRAFFIC_FLOW_URL, params)\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache and os.path.exists(cache_file):\n",
    "        print(f\"Loading from cache: {cache_file}\")\n",
    "        with open(cache_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # Make API request\n",
    "    print(f\"Fetching traffic flow data from API...\")\n",
    "    response = requests.get(TRAFFIC_FLOW_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Cache the response\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"Data cached to: {cache_file}\")\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def fetch_traffic_incidents(bbox, api_key, use_cache=True):\n",
    "    \"\"\"\n",
    "    Fetch traffic incidents for a bounding box\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox: tuple (west, south, east, north) in WGS84 coordinates\n",
    "    - api_key: HERE API key\n",
    "    - use_cache: whether to use cached data\n",
    "    \n",
    "    Returns:\n",
    "    - JSON response with traffic incident data\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'apiKey': api_key,\n",
    "        'in': f'bbox:{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}',\n",
    "        'locationReferencing': 'shape'\n",
    "    }\n",
    "    \n",
    "    cache_file = get_cache_filename(TRAFFIC_INCIDENTS_URL, params)\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache and os.path.exists(cache_file):\n",
    "        print(f\"Loading from cache: {cache_file}\")\n",
    "        with open(cache_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # Make API request\n",
    "    print(f\"Fetching traffic incidents from API...\")\n",
    "    response = requests.get(TRAFFIC_INCIDENTS_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Cache the response\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"Data cached to: {cache_file}\")\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ede91",
   "metadata": {},
   "source": [
    "## Step 5: Define Area of Interest\n",
    "\n",
    "Define the bounding box for the area you want to fetch traffic data. Example coordinates for central Kuala Lumpur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f140ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected location: Kuala Lumpur\n",
      "Bounding box: West=101.67, South=3.13, East=101.7, North=3.16\n"
     ]
    }
   ],
   "source": [
    "# Define bounding box: (west, south, east, north)\n",
    "# Example: Central Kuala Lumpur\n",
    "bbox_kl = (101.67, 3.13, 101.70, 3.16)\n",
    "\n",
    "# Example: Singapore\n",
    "bbox_singapore = (103.80, 1.28, 103.86, 1.32)\n",
    "\n",
    "# Choose which area to query\n",
    "selected_bbox = bbox_kl\n",
    "location_name = \"Kuala Lumpur\"\n",
    "\n",
    "print(f\"Selected location: {location_name}\")\n",
    "print(f\"Bounding box: West={selected_bbox[0]}, South={selected_bbox[1]}, East={selected_bbox[2]}, North={selected_bbox[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dd079",
   "metadata": {},
   "source": [
    "## Step 6: Fetch Traffic Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c650d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: cache/c490e0e07c8c89cd20defd442bde397c878fbe6c.json\n",
      "\n",
      "Traffic Flow Data Retrieved!\n",
      "Number of results: 337\n",
      "\n",
      "Example result structure:\n",
      "{\n",
      "  \"location\": {\n",
      "    \"description\": \"Jalan Tugu\",\n",
      "    \"length\": 408.0,\n",
      "    \"shape\": {\n",
      "      \"links\": [\n",
      "        {\n",
      "          \"points\": [\n",
      "            {\n",
      "              \"lat\": 3.14643,\n",
      "              \"lng\": 101.68846\n",
      "            },\n",
      "            {\n",
      "              \"lat\": 3.14628,\n",
      "              \"lng\": 101.68863\n",
      "            }\n",
      "          ],\n",
      "          \"length\": 25.0,\n",
      "          \"functionalClass\": 4\n",
      "        },\n",
      "        {\n",
      "          \"points\": [\n",
      "            {\n",
      "              \"lat\": 3.14628,\n",
      "              \"lng\": 101.688\n"
     ]
    }
   ],
   "source": [
    "# Fetch traffic flow data\n",
    "traffic_flow_data = fetch_traffic_flow(selected_bbox, HERE_API_KEY, use_cache=True)\n",
    "\n",
    "if traffic_flow_data:\n",
    "    print(f\"\\nTraffic Flow Data Retrieved!\")\n",
    "    print(f\"Number of results: {len(traffic_flow_data.get('results', []))}\")\n",
    "    \n",
    "    # Display first result as example\n",
    "    if traffic_flow_data.get('results'):\n",
    "        print(\"\\nExample result structure:\")\n",
    "        print(json.dumps(traffic_flow_data['results'][0], indent=2)[:500])\n",
    "else:\n",
    "    print(\"Failed to fetch traffic flow data. Check your API key and internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e45bf0",
   "metadata": {},
   "source": [
    "## Step 7: Parse Traffic Flow Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b651ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic Flow DataFrame created with 337 records\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 337 entries, 0 to 336\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   location_description  337 non-null    object \n",
      " 1   speed                 336 non-null    float64\n",
      " 2   speed_limit           0 non-null      object \n",
      " 3   jam_factor            337 non-null    float64\n",
      " 4   confidence            336 non-null    float64\n",
      " 5   free_flow_speed       0 non-null      object \n",
      " 6   traversability        337 non-null    object \n",
      " 7   geometry              337 non-null    object \n",
      "dtypes: float64(3), object(5)\n",
      "memory usage: 21.2+ KB\n",
      "None\n",
      "\n",
      "First few records:\n",
      "      location_description      speed speed_limit  jam_factor  confidence  \\\n",
      "0               Jalan Tugu   8.333334        None         0.0        0.70   \n",
      "1           Jalan Kinabalu   8.333334        None         3.3        0.90   \n",
      "2        Jalan Ara (South)   8.611112        None         3.1        0.99   \n",
      "3          Jalan Damansara  13.888889        None         0.0        0.93   \n",
      "4  KUALA LUMPUR NORTHBOUND  12.500000        None         0.3        0.88   \n",
      "\n",
      "  free_flow_speed traversability  \\\n",
      "0            None           open   \n",
      "1            None           open   \n",
      "2            None           open   \n",
      "3            None           open   \n",
      "4            None           open   \n",
      "\n",
      "                                            geometry  \n",
      "0  [(101.68846, 3.14643), (101.68863, 3.14628), (...  \n",
      "1       [(101.69294, 3.14297), (101.69301, 3.14281)]  \n",
      "2  [(101.6727, 3.13598), (101.67265, 3.13506), (1...  \n",
      "3  [(101.69354, 3.13785), (101.69329, 3.13768), (...  \n",
      "4  [(101.68481, 3.13818), (101.68488, 3.13812), (...  \n",
      "\n",
      "Traffic Statistics:\n",
      "Average Speed: 8.64 km/h\n",
      "Average Jam Factor: 2.34\n",
      "Average Free Flow Speed: nan km/h\n"
     ]
    }
   ],
   "source": [
    "def parse_traffic_flow_to_dataframe(traffic_data):\n",
    "    \"\"\"Convert traffic flow JSON to pandas DataFrame\"\"\"\n",
    "    \n",
    "    if not traffic_data or 'results' not in traffic_data:\n",
    "        return None\n",
    "    \n",
    "    records = []\n",
    "    for result in traffic_data['results']:\n",
    "        location = result.get('location', {})\n",
    "        current_flow = result.get('currentFlow', {})\n",
    "        \n",
    "        record = {\n",
    "            'location_description': location.get('description', ''),\n",
    "            'speed': current_flow.get('speed', None),\n",
    "            'speed_limit': current_flow.get('speedLimit', None),\n",
    "            'jam_factor': current_flow.get('jamFactor', None),\n",
    "            'confidence': current_flow.get('confidence', None),\n",
    "            'free_flow_speed': current_flow.get('freeFlowSpeed', None),\n",
    "            'traversability': current_flow.get('traversability', ''),\n",
    "        }\n",
    "        \n",
    "        # Extract geometry if available\n",
    "        if 'shape' in location:\n",
    "            shape = location['shape']\n",
    "            if 'links' in shape:\n",
    "                # Extract coordinates from links\n",
    "                coords = []\n",
    "                for link in shape['links']:\n",
    "                    if 'points' in link:\n",
    "                        for point in link['points']:\n",
    "                            coords.append((point.get('lng'), point.get('lat')))\n",
    "                record['geometry'] = coords\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Parse the data\n",
    "if traffic_flow_data:\n",
    "    flow_df = parse_traffic_flow_to_dataframe(traffic_flow_data)\n",
    "    \n",
    "    if flow_df is not None:\n",
    "        print(f\"Traffic Flow DataFrame created with {len(flow_df)} records\")\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        print(flow_df.info())\n",
    "        print(\"\\nFirst few records:\")\n",
    "        print(flow_df.head())\n",
    "        \n",
    "        # Display statistics\n",
    "        print(\"\\nTraffic Statistics:\")\n",
    "        print(f\"Average Speed: {flow_df['speed'].mean():.2f} km/h\")\n",
    "        print(f\"Average Jam Factor: {flow_df['jam_factor'].mean():.2f}\")\n",
    "        print(f\"Average Free Flow Speed: {flow_df['free_flow_speed'].mean():.2f} km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce478",
   "metadata": {},
   "source": [
    "## Step 8: Fetch Traffic Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef8fe5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: cache/ba32628a8125a8b66479bd721678955754eae4b0.json\n",
      "\n",
      "Traffic Incidents Data Retrieved!\n",
      "Number of incidents: 1\n",
      "\n",
      "Example incident structure:\n",
      "{\n",
      "  \"location\": {\n",
      "    \"length\": 105.0,\n",
      "    \"shape\": {\n",
      "      \"links\": [\n",
      "        {\n",
      "          \"points\": [\n",
      "            {\n",
      "              \"lat\": 3.15014,\n",
      "              \"lng\": 101.69454\n",
      "            },\n",
      "            {\n",
      "              \"lat\": 3.15064,\n",
      "              \"lng\": 101.69492\n",
      "            }\n",
      "          ],\n",
      "          \"length\": 70.0,\n",
      "          \"functionalClass\": 3\n",
      "        },\n",
      "        {\n",
      "          \"points\": [\n",
      "            {\n",
      "              \"lat\": 3.15064,\n",
      "              \"lng\": 101.69492\n",
      "            },\n",
      "            {\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Fetch traffic incidents\n",
    "traffic_incidents_data = fetch_traffic_incidents(selected_bbox, HERE_API_KEY, use_cache=True)\n",
    "\n",
    "if traffic_incidents_data:\n",
    "    print(f\"\\nTraffic Incidents Data Retrieved!\")\n",
    "    print(f\"Number of incidents: {len(traffic_incidents_data.get('results', []))}\")\n",
    "    \n",
    "    # Display first incident as example\n",
    "    if traffic_incidents_data.get('results'):\n",
    "        print(\"\\nExample incident structure:\")\n",
    "        print(json.dumps(traffic_incidents_data['results'][0], indent=2)[:500])\n",
    "else:\n",
    "    print(\"No incidents found or failed to fetch data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f9beb",
   "metadata": {},
   "source": [
    "## Step 9: Parse Traffic Incidents into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic Incidents DataFrame created with 1 records\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   incident_id  1 non-null      object\n",
      " 1   original_id  1 non-null      object\n",
      " 2   type         1 non-null      object\n",
      " 3   description  1 non-null      object\n",
      " 4   criticality  1 non-null      object\n",
      " 5   start_time   1 non-null      object\n",
      " 6   end_time     1 non-null      object\n",
      " 7   entry_time   1 non-null      object\n",
      " 8   geometry     1 non-null      object\n",
      "dtypes: object(9)\n",
      "memory usage: 204.0+ bytes\n",
      "None\n",
      "\n",
      "First few incidents:\n",
      "  incident_id original_id         type                          description  \\\n",
      "0                          roadClosure  Closed at Jalan Tun Perak - Closed.   \n",
      "\n",
      "  criticality            start_time              end_time  \\\n",
      "0    critical  2025-11-12T15:32:35Z  2025-11-14T03:32:35Z   \n",
      "\n",
      "             entry_time                                           geometry  \n",
      "0  2025-11-12T15:32:35Z  [(101.69454, 3.15014), (101.69492, 3.15064), (...  \n",
      "\n",
      "Incident Types:\n",
      "type\n",
      "roadClosure    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def parse_incidents_to_dataframe(incidents_data):\n",
    "    \"\"\"Convert traffic incidents JSON to pandas DataFrame\"\"\"\n",
    "    \n",
    "    if not incidents_data or 'results' not in incidents_data:\n",
    "        return None\n",
    "    \n",
    "    records = []\n",
    "    for incident in incidents_data['results']:\n",
    "        location = incident.get('location', {})\n",
    "        incident_details = incident.get('incidentDetails', {})\n",
    "        \n",
    "        record = {\n",
    "            'incident_id': incident.get('incidentId', ''),\n",
    "            'original_id': incident.get('originalId', ''),\n",
    "            'type': incident_details.get('type', ''),\n",
    "            'description': incident_details.get('description', {}).get('value', ''),\n",
    "            'criticality': incident_details.get('criticality', ''),\n",
    "            'start_time': incident_details.get('startTime', ''),\n",
    "            'end_time': incident_details.get('endTime', ''),\n",
    "            'entry_time': incident_details.get('entryTime', ''),\n",
    "        }\n",
    "        \n",
    "        # Extract location coordinates\n",
    "        if 'shape' in location:\n",
    "            shape = location['shape']\n",
    "            if 'links' in shape:\n",
    "                coords = []\n",
    "                for link in shape['links']:\n",
    "                    if 'points' in link:\n",
    "                        for point in link['points']:\n",
    "                            coords.append((point.get('lng'), point.get('lat')))\n",
    "                record['geometry'] = coords\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Parse incidents data\n",
    "if traffic_incidents_data:\n",
    "    incidents_df = parse_incidents_to_dataframe(traffic_incidents_data)\n",
    "    \n",
    "    if incidents_df is not None and len(incidents_df) > 0:\n",
    "        print(f\"Traffic Incidents DataFrame created with {len(incidents_df)} records\")\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        print(incidents_df.info())\n",
    "        print(\"\\nFirst few incidents:\")\n",
    "        print(incidents_df.head())\n",
    "        \n",
    "        # Display incident type counts\n",
    "        print(\"\\nIncident Types:\")\n",
    "        print(incidents_df['type'].value_counts())\n",
    "    else:\n",
    "        print(\"No incidents to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342bf743",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Traffic Data on a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4d8bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to: traffic_map_Kuala_Lumpur.html\n"
     ]
    }
   ],
   "source": [
    "def visualize_traffic_on_map(flow_df, incidents_df, bbox, location_name):\n",
    "    \"\"\"Create an interactive map with traffic flow and incidents\"\"\"\n",
    "    \n",
    "    # Calculate center of bounding box\n",
    "    center_lat = (bbox[1] + bbox[3]) / 2\n",
    "    center_lon = (bbox[0] + bbox[2]) / 2\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n",
    "    \n",
    "    # Add traffic flow lines\n",
    "    if flow_df is not None and 'geometry' in flow_df.columns:\n",
    "        for idx, row in flow_df.iterrows():\n",
    "            if row['geometry'] and len(row['geometry']) > 0:\n",
    "                # Determine color based on jam factor\n",
    "                jam_factor = row['jam_factor'] if pd.notna(row['jam_factor']) else 0\n",
    "                if jam_factor < 2:\n",
    "                    color = 'green'  # Free flow\n",
    "                elif jam_factor < 4:\n",
    "                    color = 'yellow'  # Moderate\n",
    "                elif jam_factor < 8:\n",
    "                    color = 'orange'  # Slow\n",
    "                else:\n",
    "                    color = 'red'  # Congested\n",
    "                \n",
    "                # Convert geometry to lat/lon format for folium\n",
    "                coords = [(lat, lon) for lon, lat in row['geometry']]\n",
    "                \n",
    "                # Create popup with traffic info (handle None values)\n",
    "                speed_text = f\"{row['speed']:.1f}\" if pd.notna(row['speed']) else \"N/A\"\n",
    "                speed_limit_text = f\"{row['speed_limit']}\" if pd.notna(row['speed_limit']) else \"N/A\"\n",
    "                jam_factor_text = f\"{row['jam_factor']:.1f}\" if pd.notna(row['jam_factor']) else \"N/A\"\n",
    "                free_flow_text = f\"{row['free_flow_speed']:.1f}\" if pd.notna(row['free_flow_speed']) else \"N/A\"\n",
    "                \n",
    "                popup_text = f\"\"\"\n",
    "                <b>Location:</b> {row['location_description']}<br>\n",
    "                <b>Speed:</b> {speed_text} km/h<br>\n",
    "                <b>Speed Limit:</b> {speed_limit_text} km/h<br>\n",
    "                <b>Jam Factor:</b> {jam_factor_text}<br>\n",
    "                <b>Free Flow Speed:</b> {free_flow_text} km/h\n",
    "                \"\"\"\n",
    "                \n",
    "                folium.PolyLine(\n",
    "                    coords,\n",
    "                    color=color,\n",
    "                    weight=3,\n",
    "                    opacity=0.7,\n",
    "                    popup=folium.Popup(popup_text, max_width=300)\n",
    "                ).add_to(m)\n",
    "    \n",
    "    # Add traffic incidents\n",
    "    if incidents_df is not None and len(incidents_df) > 0 and 'geometry' in incidents_df.columns:\n",
    "        for idx, row in incidents_df.iterrows():\n",
    "            if row['geometry'] and len(row['geometry']) > 0:\n",
    "                # Use first coordinate as marker location\n",
    "                lat, lon = row['geometry'][0][1], row['geometry'][0][0]\n",
    "                \n",
    "                # Create popup with incident info\n",
    "                popup_text = f\"\"\"\n",
    "                <b>Type:</b> {row['type']}<br>\n",
    "                <b>Description:</b> {row['description']}<br>\n",
    "                <b>Criticality:</b> {row['criticality']}<br>\n",
    "                <b>Start Time:</b> {row['start_time']}<br>\n",
    "                \"\"\"\n",
    "                \n",
    "                folium.Marker(\n",
    "                    [lat, lon],\n",
    "                    popup=folium.Popup(popup_text, max_width=300),\n",
    "                    icon=folium.Icon(color='red', icon='exclamation-triangle', prefix='fa')\n",
    "                ).add_to(m)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; right: 50px; width: 180px; height: 150px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:14px; padding: 10px\">\n",
    "    <p><strong>Traffic Flow</strong></p>\n",
    "    <p><span style=\"color:green;\">&#9632;</span> Free Flow (JF < 2)</p>\n",
    "    <p><span style=\"color:yellow;\">&#9632;</span> Moderate (JF 2-4)</p>\n",
    "    <p><span style=\"color:orange;\">&#9632;</span> Slow (JF 4-8)</p>\n",
    "    <p><span style=\"color:red;\">&#9632;</span> Congested (JF > 8)</p>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create the map\n",
    "if 'flow_df' in locals():\n",
    "    traffic_map = visualize_traffic_on_map(\n",
    "        flow_df if 'flow_df' in locals() else None,\n",
    "        incidents_df if 'incidents_df' in locals() else None,\n",
    "        selected_bbox,\n",
    "        location_name\n",
    "    )\n",
    "    \n",
    "    # Save map\n",
    "    map_filename = f\"traffic_map_{location_name.replace(' ', '_')}.html\"\n",
    "    traffic_map.save(map_filename)\n",
    "    print(f\"Map saved to: {map_filename}\")\n",
    "    \n",
    "    # Display map\n",
    "    traffic_map\n",
    "else:\n",
    "    print(\"No traffic data available to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10a586",
   "metadata": {},
   "source": [
    "## Step 11: Save Data for Merging with Map Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af983a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save traffic flow data\n",
    "if 'flow_df' in locals() and flow_df is not None:\n",
    "    flow_filename = f\"traffic_flow_{location_name.replace(' ', '_')}.csv\"\n",
    "    flow_df.to_csv(flow_filename, index=False)\n",
    "    print(f\"Traffic flow data saved to: {flow_filename}\")\n",
    "\n",
    "# Save traffic incidents data\n",
    "if 'incidents_df' in locals() and incidents_df is not None and len(incidents_df) > 0:\n",
    "    incidents_filename = f\"traffic_incidents_{location_name.replace(' ', '_')}.csv\"\n",
    "    incidents_df.to_csv(incidents_filename, index=False)\n",
    "    print(f\"Traffic incidents data saved to: {incidents_filename}\")\n",
    "\n",
    "# Also save as GeoJSON for easier merging with map data\n",
    "if 'flow_df' in locals() and flow_df is not None and 'geometry' in flow_df.columns:\n",
    "    # Convert to GeoDataFrame\n",
    "    from shapely.geometry import LineString\n",
    "    \n",
    "    geometries = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in flow_df.iterrows():\n",
    "        if row['geometry'] and len(row['geometry']) > 1:\n",
    "            try:\n",
    "                line = LineString(row['geometry'])\n",
    "                geometries.append(line)\n",
    "                valid_indices.append(idx)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if geometries:\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            flow_df.loc[valid_indices].drop(columns=['geometry']),\n",
    "            geometry=geometries,\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "        \n",
    "        geojson_filename = f\"traffic_flow_{location_name.replace(' ', '_')}.geojson\"\n",
    "        gdf.to_file(geojson_filename, driver='GeoJSON')\n",
    "        print(f\"Traffic flow GeoJSON saved to: {geojson_filename}\")\n",
    "\n",
    "print(\"\\nData saved and ready to merge with map data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75a677",
   "metadata": {},
   "source": [
    "## Step 12: Summary Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if 'flow_df' in locals() and flow_df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Speed distribution\n",
    "    axes[0, 0].hist(flow_df['speed'].dropna(), bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Speed (km/h)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Current Speeds')\n",
    "    axes[0, 0].axvline(flow_df['speed'].mean(), color='red', linestyle='--', label=f'Mean: {flow_df[\"speed\"].mean():.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Jam factor distribution\n",
    "    axes[0, 1].hist(flow_df['jam_factor'].dropna(), bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Jam Factor')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Jam Factors')\n",
    "    axes[0, 1].axvline(flow_df['jam_factor'].mean(), color='red', linestyle='--', label=f'Mean: {flow_df[\"jam_factor\"].mean():.2f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Speed vs Speed Limit\n",
    "    axes[1, 0].scatter(flow_df['speed_limit'], flow_df['speed'], alpha=0.5)\n",
    "    axes[1, 0].plot([0, flow_df['speed_limit'].max()], [0, flow_df['speed_limit'].max()], 'r--', label='Equal line')\n",
    "    axes[1, 0].set_xlabel('Speed Limit (km/h)')\n",
    "    axes[1, 0].set_ylabel('Current Speed (km/h)')\n",
    "    axes[1, 0].set_title('Current Speed vs Speed Limit')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Traffic congestion categories\n",
    "    flow_df['congestion'] = pd.cut(flow_df['jam_factor'], \n",
    "                                    bins=[0, 2, 4, 8, 10], \n",
    "                                    labels=['Free Flow', 'Moderate', 'Slow', 'Congested'])\n",
    "    congestion_counts = flow_df['congestion'].value_counts()\n",
    "    axes[1, 1].bar(congestion_counts.index.astype(str), congestion_counts.values, \n",
    "                   color=['green', 'yellow', 'orange', 'red'])\n",
    "    axes[1, 1].set_xlabel('Traffic Condition')\n",
    "    axes[1, 1].set_ylabel('Number of Road Segments')\n",
    "    axes[1, 1].set_title('Traffic Congestion Categories')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'traffic_analysis_{location_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: traffic_analysis_{location_name.replace(' ', '_')}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c3941",
   "metadata": {},
   "source": [
    "## Next Steps: Merging with Map Data\n",
    "\n",
    "To merge this traffic data with your OSM map data:\n",
    "\n",
    "1. **Spatial Join**: Use GeoPandas to perform a spatial join between the traffic flow GeoJSON and your road network from OSM\n",
    "2. **Match by Coordinates**: The traffic flow geometry (LineStrings) can be matched with OSM road segments\n",
    "3. **Attribute Transfer**: Transfer traffic attributes (speed, jam_factor) to the corresponding road segments\n",
    "\n",
    "Example code for merging:\n",
    "```python\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load your OSM road network\n",
    "G = ox.graph_from_place(\"Kuala Lumpur, Malaysia\", network_type='drive')\n",
    "edges = ox.graph_to_gdfs(G, nodes=False)\n",
    "\n",
    "# Load traffic data\n",
    "traffic_gdf = gpd.read_file(\"traffic_flow_Kuala_Lumpur.geojson\")\n",
    "\n",
    "# Perform spatial join (nearest neighbor)\n",
    "merged = gpd.sjoin_nearest(edges, traffic_gdf, how='left', max_distance=0.001)\n",
    "\n",
    "# Now you have road segments with traffic data!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4fb2f",
   "metadata": {},
   "source": [
    "## Step 13: Hourly Batch Data Collection for Historical Analysis\n",
    "\n",
    "This section sets up automated hourly data collection to build a historical traffic database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d8dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch collection function defined!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "\n",
    "def collect_traffic_data_with_timestamp(bbox, api_key, location_name):\n",
    "    \"\"\"\n",
    "    Collect traffic data and add timestamp for historical tracking\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (flow_df, incidents_df) with timestamp column added\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now()\n",
    "    \n",
    "    # Fetch traffic flow data (use_cache=False to get fresh data)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Collecting data at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    flow_data = fetch_traffic_flow(bbox, api_key, use_cache=False)\n",
    "    incidents_data = fetch_traffic_incidents(bbox, api_key, use_cache=False)\n",
    "    \n",
    "    # Parse to dataframes\n",
    "    flow_df = None\n",
    "    incidents_df = None\n",
    "    \n",
    "    if flow_data:\n",
    "        flow_df = parse_traffic_flow_to_dataframe(flow_data)\n",
    "        if flow_df is not None:\n",
    "            flow_df['timestamp'] = timestamp\n",
    "            flow_df['hour'] = timestamp.hour\n",
    "            flow_df['day_of_week'] = timestamp.strftime('%A')\n",
    "            flow_df['date'] = timestamp.date()\n",
    "            print(f\"✓ Collected {len(flow_df)} traffic flow records\")\n",
    "    \n",
    "    if incidents_data:\n",
    "        incidents_df = parse_incidents_to_dataframe(incidents_data)\n",
    "        if incidents_df is not None and len(incidents_df) > 0:\n",
    "            incidents_df['timestamp'] = timestamp\n",
    "            incidents_df['hour'] = timestamp.hour\n",
    "            incidents_df['day_of_week'] = timestamp.strftime('%A')\n",
    "            incidents_df['date'] = timestamp.date()\n",
    "            print(f\"✓ Collected {len(incidents_df)} traffic incident records\")\n",
    "        else:\n",
    "            print(\"✓ No incidents at this time\")\n",
    "    \n",
    "    return flow_df, incidents_df\n",
    "\n",
    "print(\"Batch collection function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b11e04",
   "metadata": {},
   "source": [
    "### Option 1: Save to SQLite Database (Recommended for Historical Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b56376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sqlite(flow_df, incidents_df, db_name='traffic_historical.db'):\n",
    "    \"\"\"\n",
    "    Save traffic data to SQLite database for historical analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - flow_df: DataFrame with traffic flow data\n",
    "    - incidents_df: DataFrame with traffic incidents data\n",
    "    - db_name: Name of SQLite database file\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    try:\n",
    "        # Save traffic flow data\n",
    "        if flow_df is not None and len(flow_df) > 0:\n",
    "            # Convert geometry to string for storage\n",
    "            flow_df_copy = flow_df.copy()\n",
    "            if 'geometry' in flow_df_copy.columns:\n",
    "                flow_df_copy['geometry'] = flow_df_copy['geometry'].astype(str)\n",
    "            \n",
    "            # Check if table exists and has correct schema\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='traffic_flow'\")\n",
    "            table_exists = cursor.fetchone() is not None\n",
    "            \n",
    "            if table_exists:\n",
    "                # Check if timestamp column exists\n",
    "                cursor.execute(\"PRAGMA table_info(traffic_flow)\")\n",
    "                columns = [row[1] for row in cursor.fetchall()]\n",
    "                if 'timestamp' not in columns:\n",
    "                    print(\"⚠ Warning: Existing table schema is outdated. Dropping and recreating table...\")\n",
    "                    cursor.execute(\"DROP TABLE traffic_flow\")\n",
    "                    conn.commit()\n",
    "                    table_exists = False\n",
    "            \n",
    "            # Use 'replace' for first insert to create table, then 'append'\n",
    "            if_exists = 'replace' if not table_exists else 'append'\n",
    "            flow_df_copy.to_sql('traffic_flow', conn, if_exists=if_exists, index=False)\n",
    "            print(f\"✓ Saved {len(flow_df)} flow records to database\")\n",
    "        \n",
    "        # Save traffic incidents data\n",
    "        if incidents_df is not None and len(incidents_df) > 0:\n",
    "            incidents_df_copy = incidents_df.copy()\n",
    "            if 'geometry' in incidents_df_copy.columns:\n",
    "                incidents_df_copy['geometry'] = incidents_df_copy['geometry'].astype(str)\n",
    "            \n",
    "            # Check if table exists and has correct schema\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='traffic_incidents'\")\n",
    "            table_exists = cursor.fetchone() is not None\n",
    "            \n",
    "            if table_exists:\n",
    "                # Check if timestamp column exists\n",
    "                cursor.execute(\"PRAGMA table_info(traffic_incidents)\")\n",
    "                columns = [row[1] for row in cursor.fetchall()]\n",
    "                if 'timestamp' not in columns:\n",
    "                    print(\"⚠ Warning: Existing table schema is outdated. Dropping and recreating table...\")\n",
    "                    cursor.execute(\"DROP TABLE traffic_incidents\")\n",
    "                    conn.commit()\n",
    "                    table_exists = False\n",
    "            \n",
    "            if_exists = 'replace' if not table_exists else 'append'\n",
    "            incidents_df_copy.to_sql('traffic_incidents', conn, if_exists=if_exists, index=False)\n",
    "            print(f\"✓ Saved {len(incidents_df)} incident records to database\")\n",
    "        \n",
    "        print(f\"✓ Data saved to {db_name}\")\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Test the save function with current data\n",
    "if 'flow_df' in locals():\n",
    "    # Add timestamp columns to existing flow_df for testing\n",
    "    if 'timestamp' not in flow_df.columns:\n",
    "        timestamp = datetime.now()\n",
    "        flow_df['timestamp'] = timestamp\n",
    "        flow_df['hour'] = timestamp.hour\n",
    "        flow_df['day_of_week'] = timestamp.strftime('%A')\n",
    "        flow_df['date'] = timestamp.date()\n",
    "    \n",
    "    # Add timestamp columns to incidents_df if it exists\n",
    "    if 'incidents_df' in locals() and incidents_df is not None and 'timestamp' not in incidents_df.columns:\n",
    "        timestamp = datetime.now()\n",
    "        incidents_df['timestamp'] = timestamp\n",
    "        incidents_df['hour'] = timestamp.hour\n",
    "        incidents_df['day_of_week'] = timestamp.strftime('%A')\n",
    "        incidents_df['date'] = timestamp.date()\n",
    "    \n",
    "    save_to_sqlite(flow_df, incidents_df if 'incidents_df' in locals() else None)\n",
    "    print(\"\\nDatabase created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83c766",
   "metadata": {},
   "source": [
    "#### Troubleshooting: Reset Database if Schema Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac8d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Old database deleted. You can now save data with the new schema.\n",
      "Database reset function ready (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# If you encounter schema errors, you can manually delete the old database file\n",
    "# Uncomment and run this cell to start fresh:\n",
    "\n",
    "import os\n",
    "if os.path.exists('traffic_historical.db'):\n",
    "    os.remove('traffic_historical.db')\n",
    "    print(\"✓ Old database deleted. You can now save data with the new schema.\")\n",
    "else:\n",
    "    print(\"No database file found.\")\n",
    "\n",
    "print(\"Database reset function ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42dd43e",
   "metadata": {},
   "source": [
    "### Option 2: Save to Parquet Files (Efficient for Large Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2543d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyarrow for parquet support\n",
    "# !pip install pyarrow\n",
    "\n",
    "def save_to_parquet(flow_df, incidents_df, data_dir='historical_data'):\n",
    "    \"\"\"\n",
    "    Save traffic data to Parquet files partitioned by date and hour\n",
    "    \n",
    "    Parameters:\n",
    "    - flow_df: DataFrame with traffic flow data\n",
    "    - incidents_df: DataFrame with traffic incidents data\n",
    "    - data_dir: Directory to store parquet files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    timestamp = datetime.now()\n",
    "    date_str = timestamp.strftime('%Y-%m-%d')\n",
    "    hour_str = timestamp.strftime('%H')\n",
    "    \n",
    "    # Save traffic flow data\n",
    "    if flow_df is not None and len(flow_df) > 0:\n",
    "        flow_dir = f\"{data_dir}/flow\"\n",
    "        if not os.path.exists(flow_dir):\n",
    "            os.makedirs(flow_dir)\n",
    "        \n",
    "        flow_df_copy = flow_df.copy()\n",
    "        if 'geometry' in flow_df_copy.columns:\n",
    "            flow_df_copy['geometry'] = flow_df_copy['geometry'].astype(str)\n",
    "        \n",
    "        filename = f\"{flow_dir}/flow_{date_str}_{hour_str}.parquet\"\n",
    "        flow_df_copy.to_parquet(filename, index=False)\n",
    "        print(f\"✓ Saved flow data to {filename}\")\n",
    "    \n",
    "    # Save traffic incidents data\n",
    "    if incidents_df is not None and len(incidents_df) > 0:\n",
    "        incidents_dir = f\"{data_dir}/incidents\"\n",
    "        if not os.path.exists(incidents_dir):\n",
    "            os.makedirs(incidents_dir)\n",
    "        \n",
    "        incidents_df_copy = incidents_df.copy()\n",
    "        if 'geometry' in incidents_df_copy.columns:\n",
    "            incidents_df_copy['geometry'] = incidents_df_copy['geometry'].astype(str)\n",
    "        \n",
    "        filename = f\"{incidents_dir}/incidents_{date_str}_{hour_str}.parquet\"\n",
    "        incidents_df_copy.to_parquet(filename, index=False)\n",
    "        print(f\"✓ Saved incidents data to {filename}\")\n",
    "\n",
    "print(\"Parquet save function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7d07b",
   "metadata": {},
   "source": [
    "### Automated Hourly Collection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adac5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly collection function ready!\n",
      "\n",
      "To start collection, run:\n",
      "hourly_data_collection(selected_bbox, HERE_API_KEY, location_name, storage_method='sqlite')\n"
     ]
    }
   ],
   "source": [
    "def hourly_data_collection(bbox, api_key, location_name, storage_method='sqlite', duration_hours=None):\n",
    "    \"\"\"\n",
    "    Collect traffic data every hour\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox: Bounding box coordinates\n",
    "    - api_key: HERE API key\n",
    "    - location_name: Name of location\n",
    "    - storage_method: 'sqlite' or 'parquet'\n",
    "    - duration_hours: How many hours to collect (None for indefinite)\n",
    "    \"\"\"\n",
    "    \n",
    "    collection_count = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting hourly data collection at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Location: {location_name}\")\n",
    "    print(f\"Storage method: {storage_method}\")\n",
    "    if duration_hours:\n",
    "        print(f\"Duration: {duration_hours} hours\")\n",
    "    else:\n",
    "        print(\"Duration: Indefinite (press interrupt to stop)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Collect data\n",
    "            flow_df, incidents_df = collect_traffic_data_with_timestamp(bbox, api_key, location_name)\n",
    "            \n",
    "            # Save data\n",
    "            if storage_method == 'sqlite':\n",
    "                save_to_sqlite(flow_df, incidents_df)\n",
    "            elif storage_method == 'parquet':\n",
    "                save_to_parquet(flow_df, incidents_df)\n",
    "            else:\n",
    "                print(f\"Warning: Unknown storage method '{storage_method}'\")\n",
    "            \n",
    "            collection_count += 1\n",
    "            \n",
    "            # Check if duration limit reached\n",
    "            if duration_hours and collection_count >= duration_hours:\n",
    "                print(f\"\\n✓ Completed {collection_count} collections over {duration_hours} hours\")\n",
    "                break\n",
    "            \n",
    "            # Wait until next hour\n",
    "            now = datetime.now()\n",
    "            next_hour = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)\n",
    "            wait_seconds = (next_hour - now).total_seconds()\n",
    "            \n",
    "            print(f\"\\nNext collection at: {next_hour.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"Waiting {wait_seconds/60:.1f} minutes...\")\n",
    "            \n",
    "            time.sleep(wait_seconds)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n{'='*60}\")\n",
    "        print(f\"Collection stopped by user\")\n",
    "        print(f\"Total collections: {collection_count}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "print(\"Hourly collection function ready!\")\n",
    "print(\"\\nTo start collection, run:\")\n",
    "print(\"hourly_data_collection(selected_bbox, HERE_API_KEY, location_name, storage_method='sqlite')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd9e08",
   "metadata": {},
   "source": [
    "### Example: Collect Data for 24 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting hourly data collection at 2025-11-13 15:56:33\n",
      "Location: Kuala Lumpur\n",
      "Storage method: sqlite\n",
      "Duration: 24 hours\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Collecting data at: 2025-11-13 15:56:33\n",
      "============================================================\n",
      "Fetching traffic flow data from API...\n",
      "Data cached to: cache/c490e0e07c8c89cd20defd442bde397c878fbe6c.json\n",
      "Fetching traffic incidents from API...\n",
      "Data cached to: cache/ba32628a8125a8b66479bd721678955754eae4b0.json\n",
      "✓ Collected 337 traffic flow records\n",
      "✓ Collected 1 traffic incident records\n",
      "✓ Saved 337 flow records to database\n",
      "✓ Saved 1 incident records to database\n",
      "✓ Data saved to traffic_historical.db\n",
      "\n",
      "Next collection at: 2025-11-13 16:00:00\n",
      "Waiting 3.4 minutes...\n"
     ]
    }
   ],
   "source": [
    "# Example: Collect data every hour for 24 hours\n",
    "# Uncomment to run:\n",
    "\n",
    "hourly_data_collection(\n",
    "     bbox=selected_bbox,\n",
    "     api_key=HERE_API_KEY,\n",
    "     location_name=location_name,\n",
    "     storage_method='sqlite',  # or 'parquet'\n",
    "     duration_hours=24\n",
    ")\n",
    "\n",
    "print(\"Ready to start hourly collection!\")\n",
    "print(\"\\nUncomment the code above and run to start collecting data every hour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb0457",
   "metadata": {},
   "source": [
    "### Querying Historical Data from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f16e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query function ready!\n",
      "\n",
      "Example queries:\n",
      "1. morning_data = query_historical_data(hours=[7, 8, 9])\n",
      "2. rush_hour = query_historical_data(hours=[17, 18, 19])\n",
      "3. week_data = query_historical_data(start_date='2024-01-01', end_date='2024-01-07')\n"
     ]
    }
   ],
   "source": [
    "def query_historical_data(db_name='traffic_historical.db', table='traffic_flow', \n",
    "                         start_date=None, end_date=None, hours=None):\n",
    "    \"\"\"\n",
    "    Query historical traffic data from SQLite database\n",
    "    \n",
    "    Parameters:\n",
    "    - db_name: Name of the database file\n",
    "    - table: 'traffic_flow' or 'traffic_incidents'\n",
    "    - start_date: Start date (YYYY-MM-DD) or None for all\n",
    "    - end_date: End date (YYYY-MM-DD) or None for all\n",
    "    - hours: List of hours to filter (e.g., [7, 8, 9] for morning rush)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with historical data\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    query = f\"SELECT * FROM {table}\"\n",
    "    conditions = []\n",
    "    \n",
    "    if start_date:\n",
    "        conditions.append(f\"date >= '{start_date}'\")\n",
    "    if end_date:\n",
    "        conditions.append(f\"date <= '{end_date}'\")\n",
    "    if hours:\n",
    "        hour_list = ','.join(map(str, hours))\n",
    "        conditions.append(f\"hour IN ({hour_list})\")\n",
    "    \n",
    "    if conditions:\n",
    "        query += \" WHERE \" + \" AND \".join(conditions)\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert timestamp back to datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage (after collecting some data):\n",
    "# Get all morning rush hour data (7-9 AM)\n",
    "# morning_data = query_historical_data(hours=[7, 8, 9])\n",
    "# print(f\"Retrieved {len(morning_data)} records from morning rush hours\")\n",
    "\n",
    "# Get data for specific date range\n",
    "# week_data = query_historical_data(start_date='2024-01-01', end_date='2024-01-07')\n",
    "\n",
    "print(\"Query function ready!\")\n",
    "print(\"\\nExample queries:\")\n",
    "print(\"1. morning_data = query_historical_data(hours=[7, 8, 9])\")\n",
    "print(\"2. rush_hour = query_historical_data(hours=[17, 18, 19])\")\n",
    "print(\"3. week_data = query_historical_data(start_date='2024-01-01', end_date='2024-01-07')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42d4cf",
   "metadata": {},
   "source": [
    "## Step 14: Data Storage Recommendations\n",
    "\n",
    "Below are recommendations for storing your historical traffic data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483d8ff",
   "metadata": {},
   "source": [
    "### Storage Options Comparison\n",
    "\n",
    "| Storage Method | Pros | Cons | Best For |\n",
    "|---------------|------|------|----------|\n",
    "| **SQLite Database** | ✓ Easy querying with SQL<br>✓ Good for time-based queries<br>✓ Single file<br>✓ Built-in Python support | ✗ Slower for very large datasets<br>✗ Limited concurrent writes | Small to medium datasets (<10GB)<br>Quick prototyping<br>Ad-hoc queries |\n",
    "| **Parquet Files** | ✓ Highly compressed<br>✓ Very fast reads<br>✓ Column-oriented (efficient for analytics)<br>✓ Works with big data tools | ✗ Immutable (can't update)<br>✗ Needs explicit partitioning | Large datasets (>10GB)<br>Long-term storage<br>Analytics workloads |\n",
    "| **PostgreSQL + PostGIS** | ✓ Advanced spatial queries<br>✓ Scalable<br>✓ Concurrent access<br>✓ ACID compliance | ✗ Requires separate server<br>✗ More setup complexity | Production systems<br>Multi-user access<br>Complex spatial analysis |\n",
    "| **InfluxDB / TimescaleDB** | ✓ Optimized for time-series<br>✓ Fast aggregations<br>✓ Built-in downsampling | ✗ Requires installation<br>✗ Learning curve | Time-series analysis<br>Real-time monitoring<br>High-frequency data |\n",
    "\n",
    "### Recommended Approach\n",
    "\n",
    "For your traffic analysis project, I recommend a **hybrid approach**:\n",
    "\n",
    "1. **SQLite for initial collection** (1-7 days)\n",
    "   - Easy to set up and query\n",
    "   - Good for exploring patterns\n",
    "   - Quick prototyping of analysis\n",
    "\n",
    "2. **Parquet files for long-term storage** (weekly/monthly archives)\n",
    "   - Partition by date: `historical_data/flow/2024-01-15/`\n",
    "   - Compress older data\n",
    "   - Easy to process with Pandas/Dask\n",
    "\n",
    "3. **Upgrade to PostgreSQL+PostGIS** if:\n",
    "   - Dataset grows beyond 50GB\n",
    "   - Need real-time dashboards\n",
    "   - Multiple people accessing data\n",
    "   - Complex spatial joins with OSM data\n",
    "\n",
    "### Storage Structure Example\n",
    "\n",
    "```\n",
    "traffic_data/\n",
    "├── traffic_historical.db          # SQLite for recent data\n",
    "├── historical_data/                # Parquet archive\n",
    "│   ├── flow/\n",
    "│   │   ├── 2024-01-15/\n",
    "│   │   │   ├── flow_00.parquet\n",
    "│   │   │   ├── flow_01.parquet\n",
    "│   │   │   └── ...\n",
    "│   │   └── 2024-01-16/\n",
    "│   └── incidents/\n",
    "│       ├── 2024-01-15/\n",
    "│       └── 2024-01-16/\n",
    "└── analysis/                       # Processed/aggregated data\n",
    "    ├── hourly_averages.parquet\n",
    "    ├── daily_patterns.parquet\n",
    "    └── peak_hours.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f3f8a",
   "metadata": {},
   "source": [
    "### Data Volume Estimation\n",
    "\n",
    "Calculate expected data size for planning storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_data_size(num_flow_records, num_incident_records, duration_days):\n",
    "    \"\"\"\n",
    "    Estimate storage requirements for traffic data collection\n",
    "    \n",
    "    Parameters:\n",
    "    - num_flow_records: Average number of flow records per hour\n",
    "    - num_incident_records: Average number of incident records per hour\n",
    "    - duration_days: Number of days to collect data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Approximate sizes per record (in KB)\n",
    "    flow_record_size_kb = 0.5  # ~500 bytes per flow record\n",
    "    incident_record_size_kb = 0.8  # ~800 bytes per incident record\n",
    "    \n",
    "    hours_per_day = 24\n",
    "    total_hours = duration_days * hours_per_day\n",
    "    \n",
    "    # Calculate total records\n",
    "    total_flow_records = num_flow_records * total_hours\n",
    "    total_incident_records = num_incident_records * total_hours\n",
    "    \n",
    "    # Calculate sizes\n",
    "    flow_size_mb = (total_flow_records * flow_record_size_kb) / 1024\n",
    "    incident_size_mb = (total_incident_records * incident_record_size_kb) / 1024\n",
    "    total_size_mb = flow_size_mb + incident_size_mb\n",
    "    \n",
    "    # With compression (Parquet typically achieves 3-5x compression)\n",
    "    compressed_size_mb = total_size_mb / 4\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Storage Estimation for {duration_days} days of data collection\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nCollection Parameters:\")\n",
    "    print(f\"  - Flow records per hour: {num_flow_records:,}\")\n",
    "    print(f\"  - Incident records per hour: {num_incident_records:,}\")\n",
    "    print(f\"  - Total hours: {total_hours:,}\")\n",
    "    print(f\"\\nStorage Requirements:\")\n",
    "    print(f\"  - Traffic Flow: {flow_size_mb:.1f} MB ({total_flow_records:,} records)\")\n",
    "    print(f\"  - Traffic Incidents: {incident_size_mb:.1f} MB ({total_incident_records:,} records)\")\n",
    "    print(f\"  - Total (SQLite): ~{total_size_mb:.1f} MB\")\n",
    "    print(f\"  - Total (Parquet compressed): ~{compressed_size_mb:.1f} MB\")\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    if total_size_mb < 1000:\n",
    "        print(\"  ✓ SQLite is perfect for this data volume\")\n",
    "    elif total_size_mb < 10000:\n",
    "        print(\"  ✓ SQLite or Parquet both work well\")\n",
    "        print(\"  ✓ Consider Parquet for better compression\")\n",
    "    else:\n",
    "        print(\"  ✓ Use Parquet for optimal storage\")\n",
    "        print(\"  ✓ Consider PostgreSQL for frequent queries\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Example: Estimate for your current data\n",
    "if 'flow_df' in locals():\n",
    "    current_flow_count = len(flow_df) if flow_df is not None else 0\n",
    "    current_incident_count = len(incidents_df) if 'incidents_df' in locals() and incidents_df is not None else 0\n",
    "    \n",
    "    print(f\"Current collection has:\")\n",
    "    print(f\"  - {current_flow_count} flow records\")\n",
    "    print(f\"  - {current_incident_count} incident records\")\n",
    "    \n",
    "    # Estimate for different durations\n",
    "    for days in [7, 30, 90, 365]:\n",
    "        estimate_data_size(current_flow_count, current_incident_count, days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fa07e",
   "metadata": {},
   "source": [
    "### Advanced: Using a Task Scheduler (Alternative to Notebook Loop)\n",
    "\n",
    "For production-level data collection, consider using a task scheduler instead of running the notebook continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603a917",
   "metadata": {},
   "source": [
    "#### Option A: Windows Task Scheduler (Windows)\n",
    "\n",
    "1. Create a Python script (`collect_traffic.py`) with the collection logic\n",
    "2. Open Task Scheduler → Create Basic Task\n",
    "3. Set trigger: Daily, repeat every 1 hour\n",
    "4. Set action: Start a program → Python executable → Script path\n",
    "\n",
    "#### Option B: Cron Job (Linux/Mac)\n",
    "\n",
    "```bash\n",
    "# Edit crontab\n",
    "crontab -e\n",
    "\n",
    "# Add this line to run every hour\n",
    "0 * * * * /path/to/python /path/to/collect_traffic.py >> /path/to/logs/traffic_collection.log 2>&1\n",
    "```\n",
    "\n",
    "#### Option C: Python Script with Schedule Library\n",
    "\n",
    "Create a standalone Python script that can run in the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample standalone script content for collect_traffic.py\n",
    "# Save this as a .py file and run separately\n",
    "\n",
    "standalone_script = '''\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "HERE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "BBOX = (101.67, 3.13, 101.70, 3.16)  # Kuala Lumpur\n",
    "LOCATION_NAME = \"Kuala_Lumpur\"\n",
    "DB_NAME = \"traffic_historical.db\"\n",
    "\n",
    "# ... (copy the fetch_traffic_flow, fetch_traffic_incidents, \n",
    "#      parse functions, and save_to_sqlite from above)\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting collection at {datetime.now()}\")\n",
    "    \n",
    "    # Fetch data\n",
    "    flow_data = fetch_traffic_flow(BBOX, HERE_API_KEY, use_cache=False)\n",
    "    incidents_data = fetch_traffic_incidents(BBOX, HERE_API_KEY, use_cache=False)\n",
    "    \n",
    "    # Parse data\n",
    "    flow_df = parse_traffic_flow_to_dataframe(flow_data)\n",
    "    incidents_df = parse_incidents_to_dataframe(incidents_data)\n",
    "    \n",
    "    # Add timestamps\n",
    "    timestamp = datetime.now()\n",
    "    if flow_df is not None:\n",
    "        flow_df[\"timestamp\"] = timestamp\n",
    "        flow_df[\"hour\"] = timestamp.hour\n",
    "        flow_df[\"day_of_week\"] = timestamp.strftime(\"%A\")\n",
    "        flow_df[\"date\"] = timestamp.date()\n",
    "    \n",
    "    if incidents_df is not None:\n",
    "        incidents_df[\"timestamp\"] = timestamp\n",
    "        incidents_df[\"hour\"] = timestamp.hour\n",
    "        incidents_df[\"day_of_week\"] = timestamp.strftime(\"%A\")\n",
    "        incidents_df[\"date\"] = timestamp.date()\n",
    "    \n",
    "    # Save to database\n",
    "    save_to_sqlite(flow_df, incidents_df, DB_NAME)\n",
    "    print(f\"Collection completed at {datetime.now()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(\"Standalone script template ready!\")\n",
    "print(\"\\nTo use:\")\n",
    "print(\"1. Copy the code above to a file named 'collect_traffic.py'\")\n",
    "print(\"2. Include all necessary functions from this notebook\")\n",
    "print(\"3. Set up a task scheduler to run it hourly\")\n",
    "print(\"4. Or run: python collect_traffic.py (manually for testing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee1dd5",
   "metadata": {},
   "source": [
    "## Summary: Quick Start Guide for Historical Data Collection\n",
    "\n",
    "### Step-by-Step Instructions:\n",
    "\n",
    "1. **Run cells 1-9** to set up your environment and test a single data collection\n",
    "\n",
    "2. **Choose your storage method:**\n",
    "   - **SQLite** (recommended for starting): Good for 1-30 days of data\n",
    "   - **Parquet**: Better for long-term archives (>30 days)\n",
    "\n",
    "3. **Start hourly collection:**\n",
    "   ```python\n",
    "   # In a new cell or uncomment in Step 13:\n",
    "   hourly_data_collection(\n",
    "       bbox=selected_bbox,\n",
    "       api_key=HERE_API_KEY,\n",
    "       location_name=location_name,\n",
    "       storage_method='sqlite',  # or 'parquet'\n",
    "       duration_hours=24  # or None for continuous\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Query your historical data:**\n",
    "   ```python\n",
    "   # Get morning rush hour data\n",
    "   morning_data = query_historical_data(hours=[7, 8, 9])\n",
    "   \n",
    "   # Get evening rush hour data\n",
    "   evening_data = query_historical_data(hours=[17, 18, 19])\n",
    "   \n",
    "   # Analyze patterns\n",
    "   avg_speed_by_hour = morning_data.groupby('hour')['speed'].mean()\n",
    "   ```\n",
    "\n",
    "5. **For long-term collection**, use a task scheduler instead of keeping the notebook running\n",
    "\n",
    "### Analysis Ideas:\n",
    "\n",
    "- **Peak hours identification**: Find when traffic is worst\n",
    "- **Day-of-week patterns**: Compare weekdays vs weekends\n",
    "- **Speed predictions**: Build ML models to predict congestion\n",
    "- **Incident correlation**: See how incidents affect traffic flow\n",
    "- **Route optimization**: Find fastest routes at different times\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Collect data for at least 1 week to see daily patterns\n",
    "- Collect for 1+ month to see weekly patterns\n",
    "- Use the OSM map merging code to visualize on actual roads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
